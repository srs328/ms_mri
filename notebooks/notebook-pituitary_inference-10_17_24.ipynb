{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for running inference\n",
    "\n",
    "To convert this to a script, should have a name for the whole training task which goes in dataset.json and/or datalist.json because this will help for the task.json needed for inference.\n",
    "\n",
    "Look into the minimal amount of metaata monai requires in the form of a task.json or other struct to initialize the builder and then run the ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srs-9/.virtualenvs/monai/lib/python3.12/site-packages/ignite/handlers/checkpoint.py:17: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import platform\n",
    "import numpy as np\n",
    "import json\n",
    "from loguru import logger\n",
    "\n",
    "import mri_data\n",
    "import monai_training\n",
    "\n",
    "from monai.apps.auto3dseg import (\n",
    "    AlgoEnsembleBestN,\n",
    "    AlgoEnsembleBuilder,\n",
    "    import_bundle_algo_history,\n",
    ")\n",
    "from monai.utils.enums import AlgoKeys\n",
    "\n",
    "from reload_recursive import reload_recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_recursive(mri_data)\n",
    "reload_recursive(monai_training)\n",
    "from mri_data.file_manager import scan_3Tpioneer_bids, DataSet, filter_first_ses  # noqa: E402\n",
    "from monai_training import preprocess  # noqa: E402"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_dir = \".logs\"\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "logger.add(\n",
    "    os.path.join(log_dir, \"file_{time:%Y_%m_%d}.log\"), rotation=\"6h\", level=\"DEBUG\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit These**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Set these variables\n",
    "work_dir_name = \"cp_work_dir_pituitary1\"\n",
    "train_dataset_file_name = \"training-dataset.json\"\n",
    "prediction_postfix = \"pituitary_pred\"\n",
    "task_name = \"infer_pituitary\"\n",
    "modalities = [\"t1\"]\n",
    "save_dir = Path(\"/mnt/h/3Tpioneer_bids_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostname = platform.node()\n",
    "if hostname == \"rhinocampus\":\n",
    "    drive_root = Path(\"/media/smbshare\")\n",
    "else:\n",
    "    drive_root = Path(\"/mnt/h\")\n",
    "\n",
    "projects_root = Path(\"/home/hemondlab/Dev\")\n",
    "\n",
    "msmri_home = projects_root / \"ms_mri\"\n",
    "training_work_dirs = msmri_home / \"training_work_dirs\"\n",
    "\n",
    "# dataroot = \"/media/hemondlab/Data/3Tpioneer_bids\"\n",
    "dataroot = drive_root / \"3Tpioneer_bids\"\n",
    "work_dir = training_work_dirs / work_dir_name\n",
    "train_dataset_file = work_dir / train_dataset_file_name\n",
    "\n",
    "prediction_filename = (\n",
    "    \".\".join(sorted(modalities)) + \"_\" + prediction_postfix + \".nii.gz\"\n",
    ")\n",
    "\n",
    "taskfile_name = \"inference-task.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_exists(dataset: DataSet) -> DataSet:\n",
    "    count = 0\n",
    "    dataset_new = DataSet()\n",
    "    for scan in dataset:\n",
    "        if not (save_dir / scan.relative_path / prediction_filename).is_file():\n",
    "            dataset_new.append(scan)\n",
    "        else:\n",
    "            count += 1\n",
    "    logger.info(f\"{count} scans already have inference\")\n",
    "    return dataset_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Preparation\n",
    "\n",
    "Get all the scans that were not in the training and testing set to create the inference dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scans that were used in the training\n",
    "dataset_train, _ = preprocess.load_dataset(train_dataset_file)\n",
    "\n",
    "# dataset_train2 has the same subject/sessions that are in dataset_train but with a subset of the keys\n",
    "#   so that they can be compared to scans in the full data set when getting the set difference\n",
    "dataset_proc = preprocess.DataSetProcesser.new_dataset(\n",
    "    dataroot, scan_3Tpioneer_bids, filters=[filter_first_ses, inference_exists]\n",
    ")\n",
    "dataset_full = dataset_proc.dataset\n",
    "dataset_train2 = DataSet.dataset_like(dataset_train, [\"subid\", \"sesid\"])\n",
    "dataset_inference = DataSet.from_scans(set(dataset_full) - set(dataset_train2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the images in the inference dataset (i.e combine flair and t1 images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_proc = preprocess.DataSetProcesser(dataset_inference)\n",
    "dataset_proc.prepare_images([\"flair\", \"t1\"])\n",
    "dataset_proc.dataset.sort(key=lambda s: s.subid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for scan in dataset_proc.dataset:\n",
    "    infile: Path = scan.image_path\n",
    "    images.append({\"image\": str(infile.relative_to(dataset_proc.dataset.dataroot))})\n",
    "\n",
    "logger.info(f\"Will run inference on {len(images)} scans\")\n",
    "\n",
    "datalist = {\"testing\": images}\n",
    "\n",
    "datalist_file = work_dir / \"datalist.json\"\n",
    "with open(datalist_file, \"w\") as f:\n",
    "    json.dump(datalist, f)\n",
    "\n",
    "task = {\n",
    "    \"name\": task_name,\n",
    "    \"task\": \"segmentation\",\n",
    "    \"modality\": \"MRI\",\n",
    "    \"datalist\": str(work_dir / \"datalist.json\"),\n",
    "    \"dataroot\": str(dataroot),\n",
    "}\n",
    "\n",
    "task_file = os.path.join(work_dir, taskfile_name)\n",
    "with open(task_file, \"w\") as f:\n",
    "    json.dump(task, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cfg = task_file  # path to the task input YAML file created by the users\n",
    "\n",
    "history = import_bundle_algo_history(work_dir, only_trained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model ensemble\n",
    "n_best = 5\n",
    "builder = AlgoEnsembleBuilder(history, input_cfg)\n",
    "builder.set_ensemble_method(AlgoEnsembleBestN(n_best=n_best))\n",
    "ensemble = builder.get_ensemble()\n",
    "save_params = {\n",
    "    \"_target_\": \"SaveImage\",\n",
    "    \"output_dir\": save_dir,\n",
    "    \"data_root_dir\": dataroot,\n",
    "    \"output_postfix\": prediction_postfix,\n",
    "    \"separate_folder\": False,\n",
    "}\n",
    "\n",
    "pred = ensemble(pred_param={\"image_save_func\": save_params})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
