{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for running inference\n",
    "\n",
    "To convert this to a script, should have a name for the whole training task which goes in dataset.json and/or datalist.json because this will help for the task.json needed for inference.\n",
    "\n",
    "Look into the minimal amount of metaata monai requires in the form of a task.json or other struct to initialize the builder and then run the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import mri_data\n",
    "import monai_training\n",
    "\n",
    "from monai.apps.auto3dseg import (\n",
    "    AlgoEnsembleBestN,\n",
    "    AlgoEnsembleBuilder,\n",
    "    import_bundle_algo_history,\n",
    ")\n",
    "from monai.utils.enums import AlgoKeys\n",
    "\n",
    "# from reload_recursive import reload_recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload_recursive(mri_data)\n",
    "# reload_recursive(monai_training)\n",
    "from mri_data import utils\n",
    "from mri_data.file_manager import scan_3Tpioneer_bids, Scan, DataSet\n",
    "from monai_training import preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit These**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Set these variables\n",
    "work_dir_name = \"cp_work_dir_pituitary1\"\n",
    "train_dataset_file_name = \"training-dataset-desktop1.json\"\n",
    "prediction_postfix = \"pituitary_pred\"\n",
    "modalities = [\"t1\"]\n",
    "save_dir = Path(\"/media/smbshare/3Tpioneer_bids_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_root = Path(\"/home/hemondlab/Dev\")\n",
    "drive_root = Path(\"/media/smbshare\")\n",
    "\n",
    "msmri_home = projects_root / \"ms_mri\"\n",
    "training_work_dirs = msmri_home / \"training_work_dirs\"\n",
    "\n",
    "# dataroot = \"/media/hemondlab/Data/3Tpioneer_bids\"\n",
    "dataroot = drive_root / \"3Tpioneer_bids\"\n",
    "work_dir = training_work_dirs / work_dir_name\n",
    "train_dataset_file = work_dir / train_dataset_file_name\n",
    "\n",
    "prediction_filename = (\n",
    "    \".\".join(sorted(modalities)) + \"_\" + prediction_postfix + \".nii.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the scans that were not in the training and testing set to create the inference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scans that were used in the training \n",
    "dataset_train, _ = preprocess.load_dataset(train_dataset_file)\n",
    "\n",
    "# dataset_train2 has the same subject/sessions that are in dataset_train but with a subset of the keys\n",
    "#   so that they can be compared to scans in the full data set when getting the set difference\n",
    "dataset_proc = preprocess.DataSetProcesser.new_dataset(dataroot, scan_3Tpioneer_bids, filters=[\"first_ses\"])\n",
    "dataset_full = dataset_proc.dataset\n",
    "dataset_train2 = DataSet.dataset_like(dataset_train, ['subid', 'sesid'])\n",
    "dataset_inference = DataSet.from_scans(set(dataset_full) - set(dataset_train2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the images in the inference dataset (i.e combine flair and t1 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_proc = preprocess.DataSetProcesser(dataset_inference)\n",
    "dataset_proc.prepare_images([\"flair\", \"t1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for scan in dataset_proc.dataset:\n",
    "    infile: Path = scan.image_path\n",
    "    images.append({\"image\": str(infile.relative_to(dataset_proc.dataset.dataroot))})\n",
    "\n",
    "datalist = {\n",
    "    \"testing\": images\n",
    "}\n",
    "\n",
    "datalist_file = work_dir / \"datalist.json\"\n",
    "with open(datalist_file, \"w\") as f:\n",
    "    json.dump(datalist, f)\n",
    "\n",
    "task = {\n",
    "  \"name\": \"infer_pineal\",\n",
    "  \"task\": \"segmentation\",\n",
    "  \"modality\": \"MRI\",\n",
    "  \"datalist\": str(work_dir / \"datalist.json\"),\n",
    "  \"dataroot\": str(dataroot),\n",
    "}\n",
    "\n",
    "task_file = os.path.join(work_dir, \"inference-task.json\")\n",
    "with open(task_file, \"w\") as f:\n",
    "    json.dump(task, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cfg = work_dir  / \"inference-task.json\"  # path to the task input YAML file created by the users\n",
    "\n",
    "history = import_bundle_algo_history(work_dir, only_trained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model ensemble\n",
    "n_best = 5\n",
    "builder = AlgoEnsembleBuilder(history, input_cfg)\n",
    "builder.set_ensemble_method(AlgoEnsembleBestN(n_best=n_best))\n",
    "ensemble = builder.get_ensemble()\n",
    "save_params = {\n",
    "    \"_target_\": \"SaveImage\", \n",
    "    \"output_dir\": save_dir, \n",
    "    \"data_root_dir\": dataroot, \n",
    "    \"output_postfix\": \"pineal_pred\", \n",
    "    \"separate_folder\": False}\n",
    "                               \n",
    "pred = ensemble(pred_param = {\"image_save_func\": save_params})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
